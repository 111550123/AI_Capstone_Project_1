{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove BGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from spleeter.separator import Separator\n",
    "\n",
    "# Path to mp3 files\n",
    "input_directory = \"datasets/mp3_withBGM\"\n",
    "output_directory = \"datasets/wav_files\"\n",
    "\n",
    "# Initialize the Spleeter separator with the 2stems model (vocals + accompaniment)\n",
    "# This will automatically download a pretrained model\n",
    "separator = Separator('spleeter:2stems')\n",
    "\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".mp3\"):\n",
    "        file_path = os.path.join(input_directory, filename)\n",
    "        output_path = os.path.join(output_directory, filename.replace(\".mp3\", \"_vocals.wav\"))\n",
    "        \n",
    "        # Process the file to separate the vocals and accompaniment\n",
    "        print(f\"Processing {filename}...\")\n",
    "        separator.separate_to_file(file_path, output_directory)\n",
    "        \n",
    "        # After Spleeter processes the file, it will create a folder with the same name as the input file\n",
    "        # which will contain 'vocals' and 'accompaniment' audio files.\n",
    "        # Move the vocals file to the desired location\n",
    "        vocals_file_path = os.path.join(output_directory, filename.replace(\".mp3\", \"/vocals.wav\"))\n",
    "        \n",
    "        # Move the extracted vocals file to the desired location\n",
    "        os.rename(vocals_file_path, output_path)\n",
    "        \n",
    "        # Remove the accompanying files.\n",
    "        accompaniment_file_path = os.path.join(output_directory, filename.replace(\".mp3\", \"/accompaniment.wav\"))\n",
    "        if os.path.exists(accompaniment_file_path):\n",
    "            os.remove(accompaniment_file_path)\n",
    "        \n",
    "        print(f\"Saved vocals to {output_path}\")\n",
    "        \n",
    "print(\"Processing complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capture features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# Reference: https://github.com/Renovamen/Speech-Emotion-Recognition/blob/master/extract_feats/librosa.py\n",
    "\n",
    "# Directory containing .wav files\n",
    "audio_dir = 'datasets/wav_files'\n",
    "# audio_dir = 'datasets/wav_files_EI_smaller'\n",
    "# audio_dir = 'datasets/wav_files_EI_inbalanced'\n",
    "\n",
    "# Create a list to hold features for each file\n",
    "features_list = []\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(audio_dir):\n",
    "    if filename.endswith('.wav'):\n",
    "        label =  filename.split('_')[1] # [1] => E/I, [2] => S/N, [3] => T/F, [4] => P/J\n",
    "        file_path = os.path.join(audio_dir, filename)\n",
    "        \n",
    "        # Load the audio file\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        stft = np.abs(librosa.stft(y))\n",
    "        \n",
    "        # Extract features as above (e.g., MFCC, Spectral Centroid, etc.)\n",
    "        # fmin 和 fmax 对应于人类语音的最小最大基本频率\n",
    "        pitches, magnitudes = librosa.piptrack(y=y, sr=sr, S=stft, fmin=70, fmax=400)\n",
    "        pitch = []\n",
    "        for i in range(magnitudes.shape[1]):  # every time slot\n",
    "            index = magnitudes[:, 1].argmax()\n",
    "            pitch.append(pitches[index, i])\n",
    "\n",
    "        # pitch_tuning_offset = librosa.pitch_tuning(pitches)\n",
    "        pitchmean = np.mean(pitch)\n",
    "        pitchstd = np.std(pitch)\n",
    "        pitchmax = np.max(pitch)\n",
    "\n",
    "        # 频谱质心\n",
    "        cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        cent = cent / np.sum(cent)\n",
    "        meancent = np.mean(cent)\n",
    "        stdcent = np.std(cent)\n",
    "        maxcent = np.max(cent)\n",
    "\n",
    "        # # 谱平面\n",
    "        # flat = librosa.feature.spectral_flatness(y=y)\n",
    "        # meanflat = np.mean(flat)\n",
    "        # stdflat = np.std(flat)\n",
    "\n",
    "        # ottava对比\n",
    "        cont = librosa.feature.spectral_contrast(S=stft, sr=sr)\n",
    "        meancont = np.mean(cont.T, axis = 0)\n",
    "        stdcont = np.std(cont.T, axis = 0)\n",
    "        \n",
    "        rolloff = librosa.feature.spectral_rolloff(S=stft, sr=sr)  #, roll_percent=0.85\n",
    "        meanrolloff = np.mean(rolloff)\n",
    "        stdrolloff = np.std(rolloff)\n",
    "\n",
    "        # 色谱图\n",
    "        chroma = librosa.feature.chroma_stft(S=stft, sr=sr)\n",
    "        meanchroma = np.mean(chroma.T, axis=0)\n",
    "        stdchroma = np.std(chroma.T, axis=0)\n",
    "\n",
    "        # 梅尔频率\n",
    "        mel = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "        meanmel = np.mean(mel.T, axis=0)\n",
    "        stdmel = np.std(mel.T, axis=0)\n",
    "\n",
    "        # 过零率\n",
    "        zerocr = librosa.feature.zero_crossing_rate(y=y) \n",
    "        meanzerocr = np.mean(zerocr)\n",
    "        stdzerocr = np.std(zerocr)\n",
    "     \n",
    "        S, phase = librosa.magphase(stft)\n",
    "        meanMagnitude = np.mean(S)\n",
    "        stdMagnitude = np.std(S)\n",
    "        maxMagnitude = np.max(S)\n",
    "\n",
    "        # 均方根能量\n",
    "        rmse = librosa.feature.rms(S=S)[0]\n",
    "        meanrms = np.mean(rmse)\n",
    "        stdrms = np.std(rmse)\n",
    "        maxrms = np.max(rmse)\n",
    "\n",
    "        # 使用系数为50的MFCC特征\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=50).T, axis=0)\n",
    "        mfccsstd = np.std(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=50).T, axis=0)\n",
    "        mfccmax = np.max(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=50).T, axis=0)\n",
    "        \n",
    "        # tempo, beats = librosa.beat.beat_track(y=y, sr=sr)\n",
    "\n",
    "        # Store the features\n",
    "        ext_features = np.array([\n",
    "            pitchmean, pitchstd, pitchmax,\n",
    "            meancent, stdcent, maxcent, meanrolloff, stdrolloff, #, meanflat, stdflat\n",
    "            meanzerocr, stdzerocr, meanMagnitude, stdMagnitude, maxMagnitude,\n",
    "            meanrms, stdrms, maxrms\n",
    "        ])\n",
    "        \n",
    "        ext_features = np.concatenate((ext_features, meancont, stdcont, meanchroma, stdchroma, meanmel, stdmel, mfccs, mfccsstd, mfccmax))\n",
    "\n",
    "        # Append features for this file to the list\n",
    "        features_list.append([filename, ext_features, label])\n",
    "\n",
    "# # Print the extracted features for the first file\n",
    "# print(features_list[0])\n",
    "\n",
    "# Save featuees to a CSV file\n",
    "feature_path = 'datasets/features/features_EI.p'\n",
    "# feature_path = 'datasets/features/features_SN.p'\n",
    "# feature_path = 'datasets/features/features_TF.p'\n",
    "# feature_path = 'datasets/features/features_JP.p'\n",
    "# feature_path = 'datasets/features/features_EI_smaller.p'\n",
    "# feature_path = 'datasets/features/features_EI_inbalanced.p'\n",
    "pickle.dump(features_list, open(feature_path, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "feature_path = 'datasets/features/features_EI.p'\n",
    "# feature_path = 'datasets/features/features_SN.p'\n",
    "# feature_path = 'datasets/features/features_TF.p'\n",
    "# feature_path = 'datasets/features/features_JP.p'\n",
    "# feature_path = 'datasets/features/features_EI_smaller.p'\n",
    "# feature_path = 'datasets/features/features_EI_inbalanced.p'\n",
    "\n",
    "features = pd.DataFrame(\n",
    "    data = joblib.load(feature_path),\n",
    "    columns = ['file_name', 'features', 'label']\n",
    ")\n",
    "X = list(features['features'])\n",
    "y = list(features['label'])\n",
    "\n",
    "# Standardization\n",
    "scaler_path = 'models/SCALER_LIBROSA.m'\n",
    "scaler = StandardScaler().fit(X)\n",
    "joblib.dump(scaler, scaler_path)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE (imbalanced dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Fit and resample the training data (X_train, y_train)\n",
    "X_train_valid, y_train_valid = smote.fit_resample(X_train_valid, y_train_valid)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Check the original dimension = 460\n",
    "print(\"Dimension of ext_features:\", X.shape)\n",
    "\n",
    "# Apply PCA \n",
    "pca = PCA(n_components = 60)\n",
    "X_pca = pca.fit_transform(X)\n",
    "print(\"Dimension of ext_features:\", X_pca.shape)\n",
    "\n",
    "# # Explained variance ratio to understand how much information each principal component captures\n",
    "# print(\"Explained Variance Ratio by each component:\", pca.explained_variance_ratio_)\n",
    "\n",
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn import svm\n",
    "import joblib\n",
    "\n",
    "# Define classifier\n",
    "svm_model = svm.SVC(kernel='linear', C=1, random_state=42, probability=True)\n",
    "svm_model.fit(X_train, y_train)\n",
    "joblib.dump(svm_model, 'models/svm_model.pkl')\n",
    "\n",
    "y_test_pred = svm_model.predict(X_test)\n",
    "print(y_test)\n",
    "print(y_test_pred)\n",
    "test_accuracy = svm_model.score(X_test, y_test)\n",
    "print(\"test accuracy\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Confusion Matrix\n",
    "# True E / N / F / J  Fake I\n",
    "# Fake E / N / F / J  True I\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Classification Report (Precision, Recall, F1-score)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# cross-validation\n",
    "scores = cross_val_score(svm_model, X_train_valid, y_train_valid, cv=5)\n",
    "precision_scores = cross_val_score(svm_model, X_train_valid, y_train_valid, cv=5, scoring='precision_macro')\n",
    "recall_scores = cross_val_score(svm_model, X_train_valid, y_train_valid, cv=5, scoring='recall_macro')\n",
    "f1_scores = cross_val_score(svm_model, X_train_valid, y_train_valid, cv=5, scoring='f1_macro')\n",
    "\n",
    "print(scores)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "print(precision_scores)\n",
    "print(\"%0.2f precision with a standard deviation of %0.2f\" % (precision_scores.mean(), precision_scores.std()))\n",
    "print(recall_scores)\n",
    "print(\"%0.2f recall with a standard deviation of %0.2f\" % (recall_scores.mean(), recall_scores.std()))\n",
    "print(f1_scores)\n",
    "print(\"%0.2f f1 score with a standard deviation of %0.2f\" % (f1_scores.mean(), f1_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create the MLP model\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100, ), max_iter=1000, random_state=42)\n",
    "\n",
    "mlp_model.fit(X_train, y_train)\n",
    "joblib.dump(mlp_model, 'models/mlp_model.pkl')\n",
    "\n",
    "y_test_pred = mlp_model.predict(X_test)\n",
    "print(y_test)\n",
    "print(y_test_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"test accuracy\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Confusion Matrix\n",
    "# True E / N / F / J  Fake I\n",
    "# Fake E / N / F / J  True I\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Classification Report (Precision, Recall, F1-score)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# cross-validation\n",
    "scores = cross_val_score(mlp_model, X_train_valid, y_train_valid, cv=5)\n",
    "precision_scores = cross_val_score(mlp_model, X_train_valid, y_train_valid, cv=5, scoring='precision_macro')\n",
    "recall_scores = cross_val_score(mlp_model, X_train_valid, y_train_valid, cv=5, scoring='recall_macro')\n",
    "f1_scores = cross_val_score(mlp_model, X_train_valid, y_train_valid, cv=5, scoring='f1_macro')\n",
    "\n",
    "print(scores)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "print(precision_scores)\n",
    "print(\"%0.2f precision with a standard deviation of %0.2f\" % (precision_scores.mean(), precision_scores.std()))\n",
    "print(recall_scores)\n",
    "print(\"%0.2f recall with a standard deviation of %0.2f\" % (recall_scores.mean(), recall_scores.std()))\n",
    "print(f1_scores)\n",
    "print(\"%0.2f f1 score with a standard deviation of %0.2f\" % (f1_scores.mean(), f1_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import fowlkes_mallows_score\n",
    "\n",
    "# Apply K-Means clustering \n",
    "# n_cluster = 3 for E / I  &  T / F  &  J / P\n",
    "# n_cluster = 2 for S / N\n",
    "kmeans_model = KMeans(n_clusters=3, init='k-means++', random_state=42)\n",
    "kmeans_model.fit(X_train_valid)\n",
    "joblib.dump(kmeans_model, 'models/kmeans_model.pkl')\n",
    "\n",
    "y_test_pred = kmeans_model.predict(X_test)\n",
    "print(y_test)\n",
    "print(y_test_pred)\n",
    "\n",
    "# Custom mapping pick one to uncomment\n",
    "custom_mapping = {'E': 1, 'I': 0}\n",
    "# custom_mapping = {'S': 1, 'N': 0}\n",
    "# custom_mapping = {'T': 0, 'F': 1}\n",
    "# custom_mapping = {'J': 1, 'P': 2}\n",
    "\n",
    "# custom_mapping = {'E': 0, 'I': 1} # smaller\n",
    "# custom_mapping = {'E': 0, 'I': 2} # imbalance without SMOTE\n",
    "\n",
    "# Apply custom mapping\n",
    "y_test_custom = [custom_mapping[label] for label in y_test]\n",
    "\n",
    "test_accuracy = accuracy_score(y_test_custom, y_test_pred)\n",
    "print(\"test accuracy\", test_accuracy)\n",
    "\n",
    "ari = adjusted_rand_score(y_test_custom, y_test_pred)\n",
    "print(f\"Adjusted Rand Index: {ari}\")\n",
    "\n",
    "homogeneity = homogeneity_score(y_test_custom, y_test_pred)\n",
    "completeness = completeness_score(y_test_custom, y_test_pred)\n",
    "v_measure = v_measure_score(y_test_custom, y_test_pred)\n",
    "print(f\"Homogeneity: {homogeneity}\")\n",
    "print(f\"Completeness: {completeness}\")\n",
    "print(f\"V-Measure: {v_measure}\")\n",
    "\n",
    "# True I    Fake E / S\n",
    "# Fake I    True E / S\n",
    "cm = confusion_matrix(y_test_custom, y_test_pred)\n",
    "print(\"Confusion Matrix:\") \n",
    "print(cm)\n",
    "\n",
    "# silhouette = silhouette_score(X_test, y_test_pred)\n",
    "# print(f\"Silhouette Score: {silhouette}\")\n",
    "\n",
    "# fmi = fowlkes_mallows_score(y_test_custom, y_test_pred)\n",
    "# print(f\"Fowlkes-Mallows Index: {fmi}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
